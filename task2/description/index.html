<!DOCTYPE HTML>
<html lang="en">
    <head>

<title>Task 2: Regression | ICASSP 2024: Auditory EEG challenge</title>
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
<link rel="stylesheet" href="/auditory-eeg-challenge-2024/style.css" />
<meta property="og:title" content="Task 2: Regression" />
<meta property="og:description" content="Reconstruct the stimulus from the EEG" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://exporl.github.io/auditory-eeg-challenge-2024/task2/description/" /><meta property="article:section" content="task2" />



</head>
    <body class="is-preload">
        <div id="page-wrapper"><div id="header">
    <h1><a href="https://exporl.github.io/auditory-eeg-challenge-2024/" id="logo">
        Auditory EEG Challenge - ICASSP 2024
    </a></h1>

    <nav id="nav">
        <ul>
                <li class="">
                    <a href="/auditory-eeg-challenge-2024/">Home</a>
                <li class="">
                    <a href="/auditory-eeg-challenge-2024/dataset/">Dataset</a>
                <li class="">
                    <a href="/auditory-eeg-challenge-2024/registration/">Registration</a>
                <li class="">
                    <a href="#">Task 1: Match-Mismatch</a><ul>
                        <li class="">
                            <a href="/auditory-eeg-challenge-2024/task1/description">description</a>
                        <li class="">
                            <a href="/auditory-eeg-challenge-2024/task1/test_set">test set</a>
                        <li class="">
                            <a href="/auditory-eeg-challenge-2024/task1/leaderboard">leaderboard</a>
                    </ul>
                <li class="">
                    <a href="#">Task 2: Regression</a><ul>
                        <li class="">
                            <a href="/auditory-eeg-challenge-2024/task2/description">description</a>
                        <li class="">
                            <a href="/auditory-eeg-challenge-2024/task2/test_set">test set</a>
                        <li class="">
                            <a href="/auditory-eeg-challenge-2024/task2/leaderboard">leaderboard</a>
                    </ul>
        </ul>
    </nav>
</div>

            <section class="wrapper style1">
                <div class="container">
                    

                        <div class="">
                            <div id="content">
                                <article>
    <header>
        <h2>Task 2: Regression</h2>
        <p>Reconstruct the stimulus from the EEG</p>
        
        
        <ul class="tags">
</ul>

    </header><figure><img src="../../images/task_regression.png"/><figcaption>
            <h4>Schematic overview of the regression task</h4>
        </figcaption>
</figure>

<h1 id="description">Description</h1>
<p>Task 2 is a regression problem: To reconstruct the stimulus from the EEG. After reconstruction, a metric to measure the similarity is used
between the reconstructed stimulus and the original stimulus. In this task, we use the Pearson correlation.</p>
<p>For this task, the stimulus representation is defined as the envelope, as described in the preprocessing section and as defined by the provided code.
Participants are free to create their methods. However, remember that the stimulus
objective is fixed, as defined by the python file envelope.py.</p>
<p>The code for this task can be found on our <a href="https://github.com/exporl/auditory-eeg-challenge-2023-code">github repository</a></p>
<h1 id="baseline">Baseline</h1>
<p>As a first baseline, we include a linear backward model. The linear model
reconstructs the speech envelope from EEG by using a linear transformation across all
channels and a certain time (the integration window). We use an integration window of 400ms.
We train subject-dependent models, i.e. there is one model per subject.</p>
<p>As a second baseline, we include the <a href="https://www.biorxiv.org/content/10.1101/2022.09.28.509945v2">Very Large Augmented Auditory Inference (VLAAI) network</a>. The VLAAI network consists of
multiple (N) blocks, consisting of 3 different parts. The first part is a CNN stack, a convolutional neural network. This CNN consists of M=4
convolutional layers. The second part is a simple, fully connected layer of 64 units, which recombines the output filters of the CNN stack. The
last part is the output context layer. This special layers enhances the predictions made by the model up until that point, by taking the previous
context into account and combining it with the current sample. At the end of each block except the last, a skip connection is present with the
original EEG input. After the last block, the linear layer at the top of the VLAAI model combines the filters of the output context layer into a
single speech envelope. When applied to the training and test sets of the challenge, an average correlation score of 0.19 is obtained.</p>
<h1 id="evaluation-criteria">Evaluation Criteria</h1>
<p>The test set for the regression task contains half the data from test set 1 and half from test set 2. All stimuli are held-out stimuli, i.e., they
do not appear in the training set. We have split up the stimuli into several smaller segments of 60 seconds and made these available with a
segment ID and a subject ID for each segment.</p>
<p>For each segment of 60 seconds, we expect a reconstructed envelope, which will then be compared to the original envelope,
as calculated by the provided envelope script, using Pearson correlation. We will use the scipy.stats.pearsonr
function to calculate the correlation value for each segment.</p>
<p>Afterwards, the mean correlation value per subject is calculated. Then, we calculate the mean correlation values over all subjects for test set 1 and test set
2 and add both scores to obtain a final score, which will serve as the final ranking value.</p>
<p>Participants should submit a json dictionary file for the test set to an online form on our website, which contains the reconstructed
envelopes for all EEG segments. Afterward, we will calculate the score mentioned above and update this in the online leaderboard. Each
entry in the submitted dictionary should be of the form (EEG ID) : (Reconstructed Envelope).</p>
<p>A correlation value of 0 will be taken in case of
absent EEG ID entries. The reconstructed envelope should be of dimensions 1 x 3840 (i.e., 60 seconds of data at the prescribed sample rate
of 64 Hz).</p>
<figure><img src="../../images/score_regression.png"/><figcaption>
            <h4> </h4>
        </figcaption>
</figure>



                                </article>
                            </div>
                        </div>

                    
                </div>
            </section><div id="footer">
    <div class="container">
        <div class="row">
        </div>
    </div>

    <ul class="icons">
    </ul>

    <div class="copyright">
        <ul class="menu">
            
                <li>Design: <a href="https://html5up.net">HTML5 UP</a>
                <li><a href="https://github.com/half-duplex/hugo-arcana">Theme</a>
        </ul>
    </div>
</div>
</div><script src="/auditory-eeg-challenge-2024/js/jquery.min.js"></script>
<script src="/auditory-eeg-challenge-2024/js/jquery.dropotron.min.js"></script>
<script src="/auditory-eeg-challenge-2024/js/browser.min.js"></script>
<script src="/auditory-eeg-challenge-2024/js/breakpoints.min.js"></script>
<script src="/auditory-eeg-challenge-2024/js/util.js"></script>
<script src="/auditory-eeg-challenge-2024/js/main.js"></script>
</body>
</html>
